{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian methods for machine learning - Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimate\n",
    "\n",
    "Given vector of model parameters $\\theta$ and observed data $X = \\{x_1, x_2, \\ldots, x_N\\}$, we define **likelihood** $P(X|\\theta)$ as a function of the model parameters and reflects the probability of each of the values of $\\theta$ of generating the observed data.\n",
    "\n",
    "**Important**: likelihood is not a probability distribution over $\\theta$ (e.g. integral does not sum one).\n",
    "\n",
    "The **Maximum Likelihood estimate (MLE)** is the value of $\\theta$ that maximizes the probability of observing the data (i.e. the value of $\\theta$ with the highest likelihood).\n",
    "\n",
    "In this notebook we are going to derive, following the course reading, the MLE for parameter $\\mu$ for univariate and multivariate Gaussian distributions. I recommend doing this as an exercise, only looking at the notes to correct anything, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for univariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the likelihood for the Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ given point $x_i$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{N}(\\mu, \\sigma^2) & = P(x_i|\\theta) \\\\\n",
    "                           & = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\mathrm{\\exp} \\left[-\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The likelihood for a set of points is the product of the individual likelihoods:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(X|\\theta) & = \\prod^N_{i=1} P(x_i |\\theta) \\\\\n",
    "            & = \\prod^N_{i=1} \\frac{1}{\\sigma \\sqrt{2\\pi}} \\mathrm{\\exp} \\left[ -\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2\\right] \\\\\n",
    "            & =  \\left(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\right)^N\\prod^N_{i=1} \\mathrm{\\exp} \\left[ -\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2\\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the maximum of the likelihood, which is equivalent to compute the maximum of the log likelihood (as it is easier to deal with summations over products):\n",
    "\n",
    "$$\n",
    "\\mu_{MLE} =  \\underset{\\mu}{\\mathrm{argmax}} \\log P(X|\\theta) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log P(X|\\theta) & = \\log\\left(\\frac{1}{\\sigma \\sqrt{2\\pi}} \\right)^N\\sum^N_{i=1} \\mathrm{\\exp} \\left[-\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2 \\right] \\\\\n",
    "                 & = \\log 1 - \\log (\\sigma \\sqrt{2\\pi})^N + \\log \\sum^N_{i=1} \\mathrm{\\exp} \\left[-\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2 \\right] \\\\\n",
    "                 & = \\log 1 - \\log (\\sigma \\sqrt{2\\pi})^N + \\sum^N_{i=1} -\\frac{1}{2} \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2 \\\\\n",
    "                 & = \\log 1 - \\log (\\sigma \\sqrt{2\\pi})^N - \\frac{1}{2\\sigma^2} \\sum^N_{i=1}  (x_i - \\mu)^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily convert the problem into a minimization one:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{MLE} & = \\underset{\\mu}{\\mathrm{argmin}} - \\log P(X|\\theta) \\\\\n",
    "          & = \\underset{\\mu}{\\mathrm{argmin}} - \\log 1 + \\log (\\sigma \\sqrt{2\\pi})^N + \\frac{1}{2\\sigma^2} \\sum^N_{i=1}  (x_i - \\mu)^2\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function is quadratic, so it has a single minimum which we can find by computing the roots of the derivative with respect to $\\mu$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\mu} P(X|\\theta) & = 0 \\\\\n",
    "\\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{\\sigma^2} \\sum^N_{i=1} (x_i - \\mu)^2 \\right) & = 0 \\\\\n",
    "\\frac{1}{\\sigma^2} \\sum^N_{i=1} 2\\mu - 2 x_i & = 0 \\\\\n",
    "\\frac{2}{\\sigma^2} \\left( \\sum^N_{i=1} \\mu - \\sum^N_{i=1} x_i \\right) & = 0 \\\\\n",
    "\\frac{2N\\mu}{\\sigma^2} & = \\frac{2}{\\sigma^2} \\sum^N_{i=1} x_i \\implies \\mu_{MLE} = \\frac{1}{N} \\sum^N_{i=1} x_i\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE for multivariate Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the likelihood for the multivariate Gaussian distribution $\\mathcal{N}(\\mu, \\Sigma)$ (i.e. $\\mu \\in \\mathcal{R}^{k}$, $\\Sigma \\in \\mathcal{R}^{k \\times k}$) given point $x_i$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{N}(\\mu, \\Sigma) & = P(x_i|\\theta)  \\\\\n",
    "                         & = (2\\pi)^\\frac{-k}{2} |\\Sigma|^{-\\frac{1}{2}} \\mathrm{\\exp} \\left[ -\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1}(x-\\mu) \\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood for a set of points is the product of the individual likelihoods:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(X|\\theta) & = \\prod^N_{i=1} P(x_i |\\theta) \\\\\n",
    "            & = \\prod^N_{i=1} (2\\pi)^\\frac{-k}{2} |\\Sigma|^{-\\frac{1}{2}} \\mathrm{\\exp}\\left[-\\frac{1}{2} (x - \\mu)^{\\top} \\Sigma^{-1}(x_i-\\mu)\\right] \\\\\n",
    "            & = (2\\pi)^\\frac{-Nk}{2} |\\Sigma|^{-\\frac{N}{2}} \\prod^N_{i=1} \\mathrm{\\exp}\\left[-\\frac{1}{2} (x_i - \\mu)^{\\top} \\Sigma^{-1}(x-\\mu)\\right]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the log likelihood is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log P(X|\\theta) & = - \\frac{Nk}{2} \\log (2\\pi) - \\frac{N}{2} \\log |\\Sigma| + \\sum^N_{i=1} -\\frac{1}{2} (x_i - \\mu)^{\\top} \\Sigma^{-1}(x_i - \\mu) \\\\\n",
    "                 & = - \\frac{Nk}{2} \\log (2\\pi) - \\frac{N}{2} \\log |\\Sigma| - \\frac{1}{2} \\sum^N_{i=1} (x_i^{\\top} \\Sigma^{-1} - \\mu^{\\top} \\Sigma^{-1}) (x_i-\\mu) \\\\\n",
    "                 & = - \\frac{Nk}{2} \\log (2\\pi) - \\frac{N}{2} \\log |\\Sigma| - \\frac{1}{2} \\sum^N_{i=1} (x_i^{\\top} \\Sigma^{-1}x_i - x_i^{\\top} \\Sigma^{-1} \\mu - \\mu^{\\top} \\Sigma^{-1} x_i + \\mu^{\\top} \\Sigma^{-1} \\mu)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the MLE estimate for the $\\mu$ vector as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{MLE} & = \\underset{\\mu}{\\mathrm{argmax}} \\log P(X|\\theta) \\\\\n",
    "          & = \\underset{\\mu}{\\mathrm{argmin}} - \\log P(X|\\theta)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find minimum, we find the roots of the derivative with respect to $\\mu$:\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial{\\mu}} \\left[- \\log P(X|\\theta) \\right] & = 0 \\\\\n",
    "\\frac{1}{2} \\sum^N_{i=1} (x_i^{\\top} \\Sigma^{-1}x_i - x_i^{\\top} \\Sigma^{-1} \\mu - \\mu^{\\top} \\Sigma^{-1} x_i + \\mu^{\\top} \\Sigma^{-1} \\mu) & = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Given matrix derivative rule $\\frac{\\partial}{\\partial{x}} (x^{\\top}Ax) = 2Ax$ for $x \\in \\mathcal{R}^{k}$, $A \\in \\mathcal{R}^{k \\times k}$, we can express:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\sum^N_{i=1} - x_i^{T} \\Sigma^{-1} - \\Sigma^{-1} x_i + 2 \\Sigma^{-1} \\mu = 0\n",
    "$$\n",
    "\n",
    "As $\\Sigma^{-1}$ is symmetric, we can express:\n",
    "\n",
    "$$\n",
    "-x_i^{\\top}\\Sigma^{-1} = (-x_i^{\\top}\\Sigma^{-1})^{\\top} = \\Sigma^{-1} x_i\n",
    "$$\n",
    "\n",
    "Then we can rewrite previous equation as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2} \\sum^N_{i=1} - 2 x_i^{T} \\Sigma^{-1} + 2 \\Sigma^{-1} \\mu & = 0 \\\\\n",
    "\\sum^N_{i=1} \\Sigma^{-1} \\mu & = \\sum^N_{i=1} x_i^{T} \\Sigma^{-1} \\\\\n",
    "\\Sigma^{-1} \\mu N & = \\Sigma^{-1} \\sum^N_{i=1} x_i  \\implies \\mu_{MLE} = \\frac{1}{N} \\sum^N_{i=1} x_i \n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

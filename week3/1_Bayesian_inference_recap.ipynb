{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference recap\n",
    "\n",
    "Let's have a quick overview of the Bayesian methods targeted at performing full Bayesian inference on hidden latent variables.\n",
    "\n",
    "Given:\n",
    "\n",
    "- Data $\\pmb{X}$.\n",
    "- Latent variables $\\pmb{Z}$.\n",
    "- Model parameters $\\pmb{\\theta}$.\n",
    "\n",
    "The following methods are presented in ascending order of computation, which at the same time translates into a descending order of accuracy.\n",
    "\n",
    "## Full inference\n",
    "\n",
    "Performing full inference is usually prohibitive, even for small models.\n",
    "\n",
    "$$\n",
    "P(\\pmb{Z}, \\pmb{\\theta} | \\pmb{X})\n",
    "$$\n",
    "\n",
    "\n",
    "## EM algorithm\n",
    "\n",
    "EM algorithm provides a point estimate of the parameters $\\pmb{\\theta} = \\pmb{\\theta}_{MAP}$.\n",
    "\n",
    "As a reminder, we define the variational lowerbound in terms of the marginal likelihood (i.e. likelihood of parameters marginalized over the latent variables), which we want to maximize\n",
    "\n",
    "$$\n",
    "\\log \\ P(\\pmb{X} | \\pmb{\\theta}) \\ge \\mathcal{L}(\\pmb{\\theta}, q) = \\mathbb{E}_q(Z) \\log \\ \\frac{P(\\pmb{X}, Z | \\pmb{\\theta})}{q(Z)}dZ\n",
    "$$\n",
    "\n",
    "### E-step\n",
    "\n",
    "Maximizes lower bound fixing the parameters, which equals to minimizing the KL divergence between the variational distribution and the posterior:\n",
    "\n",
    "$$\n",
    "\\pmb{q} = \\underset{\\pmb{q}}{\\mathrm{argmax}} \\ \\mathcal{L}(\\pmb{\\theta}\\pmb{q}) \\longleftrightarrow \\underset{\\pmb{q}}{\\mathrm{argmin}} \\ KL[\\pmb{q}(\\pmb{Z})||P(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta})]\n",
    "$$\n",
    "\n",
    "### M-step\n",
    "\n",
    "Maximizes lower bound fixing the variational distribution, maximizing the expected value of the joint distribution under the variational distribution:\n",
    "\n",
    "$$\n",
    "\\pmb{q} = \\underset{\\pmb{\\theta}}{\\mathrm{argmax}} \\ \\mathcal{L}(\\pmb{\\theta}\\pmb{q}) \\longleftrightarrow  \\underset{\\pmb{\\theta}}{\\mathrm{argmax}} \\ \\mathbb{E}_{\\pmb{q}(\\pmb{Z})} \\log \\ P(\\pmb{X}, \\pmb{Z}|\\pmb{\\theta})\n",
    "$$\n",
    "\n",
    "## Variational EM\n",
    "\n",
    "For many models, in step EM from EM algorithm, it is yet no feasible to compute the posterior. Therefore, we need to approximate using a variational family $\\mathcal{Q}$.\n",
    "\n",
    "### Variational E-step\n",
    "\n",
    "$$\n",
    "\\pmb{q} = \\underset{\\pmb{q}}{\\mathrm{argmax}} \\ \\mathcal{L}(\\pmb{\\theta}\\pmb{q}) \\longleftrightarrow \\underset{\\pmb{q} \\in \\mathcal{Q}}{\\mathrm{argmin}} \\ KL[\\pmb{q}(\\pmb{Z})||P(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta})]\n",
    "$$\n",
    "\n",
    "### Crips EM\n",
    "\n",
    "When Variational EM cannot even be applied, we can use Crips EM, where both parameters and latent variables are approximated to a point estimate:\n",
    "\n",
    "$$\n",
    "\\pmb{\\theta} = \\pmb{\\theta}_{MAP} \\\\\n",
    "\\pmb{Z} = \\pmb{Z}_{MAP} \\\\\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational inference\n",
    "\n",
    "## Overview\n",
    "\n",
    "We have seen that computing the posterior $P(\\pmb{Z}|\\pmb{X})$ of latent variables $\\pmb{Z}$ given observed data $\\pmb{XP}$) for a model is an important task in EM (i.e. when computing the expectating of the posterior in the E step) and in all probabilistic model applications where the prior is not a conjugate for the likelihood.\n",
    "\n",
    "It may happen that it is not feasible to evaluate the posterior or compute its expectation:\n",
    "\n",
    "- High latent space dimensionality.\n",
    "- Complex for of the posterior.\n",
    "- Intractability of integrations in no closed form solutions for continuous variables.\n",
    "- Exponentially large number of configurations for discrete settings.\n",
    "- ...\n",
    "\n",
    "In those cases, we need to rely on approximations of the posterior. There are two families of approaches:\n",
    "\n",
    "- Stochastic approximations\n",
    "- Deterministic approximations\n",
    "\n",
    "|                              | Pros                              | Cons                          | Example                   |\n",
    "| ---------------------------- | --------------------------------- | ----------------------------  | ------------------------  |   \n",
    "| Stochastic approximations    | Exact results if run long enough  | Computationally expensive     | Markov chain Monte Carlo  |\n",
    "| Deterministic approximations | Scale easily                      | Cannot generate exact results | Variational inference     |\n",
    "\n",
    "We will be focusing on the family of **deterministic approximations**. Let's recall the E-step in the EM algorithm for i.i.d data $\\pmb{X}$ and latent variables $\\pmb{Z}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log P(\\pmb{X} | \\pmb{\\theta}) = \\mathcal{L}(q) + \\mathcal{KL} \\left(q(\\pmb{Z}) || P(\\pmb{Z}|\\pmb{X})\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that the parameter $\\pmb{\\theta}$ is ommitted, since latent variables are considered now as random variables and are included in $\\pmb{Z}$. Maximizing the lower wound is equivalent to minimize the KL divergence, which occur when $q$ equals to the posterior. As we assume that the posterior is not tractable, we must find an alternative: **restrict $q$ to a family of tractable distributions and then, minimize the KL divergence**.\n",
    "\n",
    "The quality of the results is conditioned by the choice of $q$. For \"wider\" family of distributions, it is more complex to compute the variational inference (e.g. imagine we decide that $\\mathcal{Q}$ is any distribution). However, if $\\mathcal{Q}$ is too narrowed, there is a high chance that the posterior does not fall in that distribution (i.e. the smaller the distribution, the greater the approximation error). The chosen family must be flexible enough but still tractable.\n",
    "\n",
    "Noe that there is **no overfiting** in this scenario: simply distributions that allows us to approach better the posterior.\n",
    "\n",
    "\n",
    "## Summing up\n",
    "\n",
    "The objective of variational inference algorithm is to estimate the posterior probability $P^{*}(\\pmb{Z})$:\n",
    "\n",
    "$$\n",
    "P^{*}(\\pmb{Z}) = P(\\pmb{Z}|\\pmb{X}) = \\frac{P(\\pmb{X}|\\pmb{Z})P(\\pmb{Z})}{P(\\pmb{X})}\n",
    "$$\n",
    "\n",
    "Two steps are involved:\n",
    "\n",
    "1. Select a family of distributions $\\mathcal{Q}$ (i.e. variational family) where to obtain lower bound $q(z)$. Example:\n",
    "\n",
    "$$\n",
    "\\mathcal{Q} \\sim \\mathcal{N}(\\pmb{\\mu}, \\pmb{\\sigma^2} \\pmb{I})\n",
    "$$\n",
    "\n",
    "2. Find best approximation $q(\\pmb{Z})$ of $P^{*}(\\pmb{Z})$ through KL divergence:\n",
    "\n",
    "$$\n",
    "q(\\pmb{Z}) = \\underset{q^{\\prime} \\in \\mathcal{Q}}{\\mathrm{argmin}} \\ \\mathcal{KL} [ q^{\\prime}(\\pmb{Z}) || P^{*}(\\pmb{Z}) ]\n",
    "$$\n",
    "\n",
    "Note that this second step is equivalent to maximizing the lower bound $\\mathcal{L}(q)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean field approximation\n",
    "\n",
    "### Formula for lower bound maximization\n",
    "\n",
    "The **mean field approximation** consists of selecting a family of distributions $\\mathcal{Q}$ such that it factorizes with respect to disjoint groups of latent variables $\\pmb{Z}_i$. Given $M$ groups:\n",
    "\n",
    "$$\n",
    "\\mathcal{Q} = \\left\\{ q | q(\\pmb{Z}) = \\prod^{M}_{i=1} q_i(\\pmb{Z}_i) \\right\\}\n",
    "$$\n",
    "\n",
    "Therefore, we assume independence between the factors groups. No additional restrictions are made. In order to optimize the lower bound, it must be optimized with respect to one of the factors at a time.\n",
    "\n",
    "Let's replace the definition into the formula of the lower bound (we are using $q_i$ to denote $q_i(\\pmb{Z}_i)$ for readability):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(q) & = \\int q \\log \\frac{P(\\pmb{X}, \\pmb{Z})}{q} d\\pmb{Z} \\\\\n",
    "               & = \\int \\prod_i^M q_i \\left[ \\log P(\\pmb{Z}, \\pmb{X}) - \\log \\prod_i^M q_i \\right] d\\pmb{Z} \\\\\n",
    "               & = \\int \\prod_i^M q_i \\log P(\\pmb{X}, \\pmb{Z}) - \\int \\prod_i^M q_i \\sum_i^M \\log q_i d\\pmb{Z} \\\\\n",
    "               & = \\int \\prod_i^M q_i \\log P(\\pmb{X}, \\pmb{Z}) - \\int \\prod_i^M q_i \\left[ \\log q_1 + \\ldots + q_k + \\ldots + q_M \\right] d\\pmb{Z} \\\\\n",
    "               & = \\int \\prod_i^M q_i \\log P(\\pmb{X}, \\pmb{Z}) - \\int \\prod_i^M q_i \\log q_k d\\pmb{Z} - \\int \\prod_i^M q_i \\sum_{i \\neq k}^M \\log q_i d\\pmb{Z} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, as we want to optimize with respect to each factor $q_k$, let's write each of the terms with respect that. First:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\int \\prod_i^M q_i \\log P(\\pmb{X}, \\pmb{Z}) & = \\int q_1 \\ldots \\int q_k \\ldots \\int q_M \\log P(\\pmb{X}, \\pmb{Z}) d\\pmb{Z}_k \\ldots d\\pmb{Z}_k \\ldots d\\pmb{Z}_M \\\\\n",
    "                                             & = \\int q_k \\left\\{ \\int \\prod_{i \\neq k}^M q_i \\log P(\\pmb{X}, \\pmb{Z}) d\\pmb{Z}_{i} \\right\\} d\\pmb{Z}_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that by definition:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_p \\left[ f(x) \\right] = \\int p(x) f(x) dx\n",
    "$$\n",
    "\n",
    "For distribution $P(q_1, \\ldots, q_{k-1}, q_{k+1}, \\ldots, q_M)$ and function $\\log P(\\pmb{X}, \\pmb{Z})$, we have:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] = \\int \\prod_{i \\neq k}^M q_i \\log P(\\pmb{X}, \\pmb{Z}) d\\pmb{Z}_{i}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\int \\prod_i^M q_i \\log P(\\pmb{X}, \\pmb{Z}) = \\int q_k \\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] d\\pmb{Z}_k\n",
    "$$\n",
    "\n",
    "For the second term, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "- \\int \\prod_i^M q_i \\log q_k d\\pmb{Z} & = - \\int q_k \\log q_k d\\pmb{Z}_k \\int \\prod_{i \\neq k}^M q_i d\\pmb{Z}_i  \\\\\n",
    "                                       & = - \\int q_k \\log q_k d\\pmb{Z}_k \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that the second integral in the first line is equal to 1 by definition of probability distributions.\n",
    "\n",
    "Finally, for the last term, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "- \\int \\prod_i^M q_i \\sum_{i \\neq k}^M \\log q_i d\\pmb{Z} & = - \\int q_k \\prod_{i \\neq k}^M q_i \\sum_{i \\neq k}^M \\log q_i d\\pmb{Z} \\\\\n",
    "                                                         & = - \\int q_k \\left\\{ \\prod_{i \\neq k}^M q_i \\sum_{i \\neq k}^M \\log q_i d\\pmb{Z}_i \\right\\} d\\pmb{Z}_k \\\\\n",
    "                                                         & = - \\int q_k C \\ d\\pmb{Z}_k \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that we can replace the term in curly brackets by a constant as it does not depend on $q_k$.\n",
    "\n",
    "Then, we can rewrite:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(q) & = \\int q_k \\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] d\\pmb{Z}_k - \\int q_k \\log q_k d\\pmb{Z}_k - \\int q_k \\mathrm{const} \\ d\\pmb{Z}_k \\\\\n",
    "               & = \\int q_k \\left( \\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] - C \\right) d\\pmb{Z}_k - \\int q_k \\log q_k d\\pmb{Z}_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, the lower bound is optimized for all possible distributions of $q_k$, leaving $\\{q_i : i \\neq k\\}$ fixed. That is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{q}_k & = \\underset{q_k}{\\mathrm{argmax}} \\ \\mathcal{L}(q) \\\\\n",
    "          & = \\underset{q_k}{\\mathrm{argmax}} \\int q_k \\left( \\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] - C \\right) d\\pmb{Z}_k - \\int q_k \\log q_k d\\pmb{Z}_k\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Note that we can rewrite the first term as a probability distribution by using the exponential (and logarithm):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] - C & =\n",
    "\\log \\exp \\left\\{ \\mathbb{E}_{q_{i \\neq j}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] - C \\right\\} \\\\\n",
    "             & = \\log \\frac{\\exp \\left(\\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] \\right)}{\\exp(\\mathrm{C})} \\\\\n",
    "             & = \\log \\hat{P}(\\pmb{X}, \\pmb{Z})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And force $\\exp(-\\mathrm{C})$ to be the normalization factor. Then, we write:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{q}_k & = \\underset{q_k}{\\mathrm{argmax}} \\ \\int q_k \\left( \\mathbb{E}_{q_{i \\neq k}} \\left[ \\log P(\\pmb{X}, \\pmb{Z}) \\right] - C \\right) d\\pmb{Z}_k - \\int q_k \\log q_k d\\pmb{Z}_k \\\\\n",
    "          & = \\underset{q_k}{\\mathrm{argmax}} \\ \\int q_k \\log \\hat{P}(\\pmb{X}, \\pmb{Z}) d\\pmb{Z}_k - \\int q_k \\log q_k d\\pmb{Z}_k \\\\\n",
    "          & = \\underset{q_k}{\\mathrm{argmax}} \\ \\int q_k \\log \\frac{\\hat{P}(\\pmb{X}, \\pmb{Z})} {q_k} d\\pmb{Z}_k \\\\\n",
    "          & = \\underset{q_k}{\\mathrm{argmax}} \\ - \\mathcal{KL} \\left[ q_k || \\hat{P}(\\pmb{X}, \\pmb{Z}) \\right] \\\\\n",
    "          & = \\underset{q_k}{\\mathrm{argmin}} \\ \\mathcal{KL} \\left[ q_k || \\hat{P}(\\pmb{X}, \\pmb{Z}) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We just have seen that maximizing the lower bound for $q_k$ is equivalent to minimizing the given KL, which occurs when $q_k = \\hat{P}(\\pmb{X}, \\pmb{K})$ (i.e. when KL is zero).\n",
    "\n",
    "In order to obtain $q$, we can iteratively optimize each of the components of $q$ independently:\n",
    "\n",
    "1. First, we initialize each of the factors $q_i$.\n",
    "1. Cycle through each of the factors replacing each $q_k$ with the nex estimate and keepint the other factors $q_{i \\neq k}$ fixed.\n",
    "1. Repeat previous step until convergence (it is guaranteed to converge as bound is convex wrt factors $q_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Explanatory [video](https://www.youtube.com/watch?v=zQEhkNpBzS4) on deriving the mean field approximation\n",
    "- 10.1 subsection from Bishop's Book.\n",
    "- Chapter 21 from \"Machine Learning: A Probabilistic Perspective\" (Murphy, 2012)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
